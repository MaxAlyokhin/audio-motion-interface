### Audio Motion Interface (AMI)

Алгоритм синтезирует звук на основе данных с датчиков движения смартфона. Предусмотрен т.н. локальный режим (когда смартфон просто проводом подключается к колонкам или наушникам) и удалённый режим (когда смартфон и колонки разнесены в пространстве, а данные идут по протоколу <a href="https://ru.wikipedia.org/wiki/WebSocket">websocket</a> через промежуточный сервер).

Обратите внимание, что алгоритм отдаёт лишь "чистый" звук, оставляя на ваше усмотрение возможность его обработки (это могут быть как "примочки", так и разнообразные DAW вроде Ableton, Cubase, FL Studio и т.д.).

### Стратегия синтеза

##### Локальный режим:

<i>Источник данных</i> и <i>управление данными</i> находятся на смартфоне (совмещены).

- смартфон → миниджек → колонки
- смартфон → миниджек → аблетон → миниджек → колонки

##### Удалённый режим:

<i>Источник данных</i> находится на смартфоне, а <i>управление данными</i> находится на удалённой машине (разнесены).

- смартфон → вебсокет → браузер → миниджек → колонки
- смартфон → вебсокет → браузер → аблетон → миниджек → колонки

<b>Плюсы:</b>

- смартфон не связан проводом с ноутбуком;
- разделение источника данных от управления данными - ноутбук лучше справится с обработкой данных;

<b>Минусы:</b>

- исполнителю не слышно ничего (можно одновременно генерить звук локально со смартфона в наушники и дополнительно дублировать данные движения на удалённую машину; по идее, тогда любой может подключиться к прослушиванию исполнения и можно сделать распределённое удалённое прослушивание откуда угодно; но тогда удалённые слушателя останутся без обработки ableton, только обработка с сервера);
- есть задержка до 100мс;

##### Совмещённый режим:

Можно сделать так: у нас должен быть локальный источник данных, удалённый источник данных и ноутбук (т.е. два смартфона и ноутбук). На ноуте открыта вкладка с удалённым источником данных + настройки ноута и смартфон с локальными настройками по проводу в ноут. Аблетон принимает два источника: по миниджеку и с браузера.

### Общая архитектура

`motion.js` занимается датчиками - акселерометром и гироскопом.
Он вешает слушатели событий движения и вызывает `audio.js` по каждому событию

`audio.js` принимает данные о движении и генерирует звук

### Запуск

Генерация ssl-сертификата

https://stackoverflow.com/questions/21397809/create-a-trusted-self-signed-ssl-cert-for-localhost-for-use-with-express-node

nodemon index

cd client

gulp

#### Название?

audio
motion
orientation
sonification
interface
algorithm
sonic

ormoson
ormosonic
orientation-motion-sonification

audio motion interface

Sonification Interface for motion and orientation

#### Как сонифицировать:

- Только акселерометр:
  параметры звука это константы, акселерометр определяет частоту;

- Комбинация датчиков:
  акселерометр определяет громкость;
  гироскоп частоту;

#### TODO:

- троттлинг-лимит (события генерятся со скоростью 16 мс; можно сделать больше для облегчения вычислений); этот параметр должен коррелировать с exponentialRampToValueAtTime, т.е. затуханием звука, чтобы всё сливалось в непрерывный звук;

- включение/отключение синтеза (active, disactive) по повторному нажатию на Run;

- данные звука: частота Гц и нота (тюнер);

- добавить отображение статуса ws на стороне смартфона ("точек синтеза не обнаружено");

- proxy-режим: локально ноут + смартфон чтобы себя слышать без задержки, далее эта инфа ретранслируется на удалённый сервер, где раздаётся слушателям;

- юзать ли отрицательные числа;

- все осцилляторы должны подключаться к одному внешнему gainNode;

- можно гироскоп привязать к бикубическому фильтру;

- играть по нотам: алгоритм должен сам приводить частоту к ближайшей ноте, сделать это спецрежимом (автотюнинг);

- гироскоп должен работать по логарифмической линейке
